{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEBLINK :- \n",
    "First website – (https://mitsloan.mit.edu/ideas-made-to-matter/machine-learning-explained )\n",
    "\n",
    "Second Website – (https://builtin.com/artificial-intelligence )\n",
    "\n",
    "Third website – (https://www.infoworld.com/article/3634602/explainable-ai-explained.html )\n",
    "\n",
    "Fourth Website- (https://www2.deloitte.com/se/sv/pages/technology/articles/part1-artificial-intelligence-defined.html )\n",
    "\n",
    "Fifth Website – (https://www.ibm.com/in-en/watson/explainable-ai )\n",
    "\n",
    "\n",
    "# PROBLEM DEFINITON :- \n",
    "Imagine you are working as a data scientist in an AI-based organization. You have to build A web scrapping model implementing word2vec which will throw similar words to the word you are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Basic Python Libraries\n",
    "import numpy as np \n",
    "import pandas as pd  \n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt   \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\nandakishore\\new folder (2)\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\nandakishore\\new folder (2)\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n"
     ]
    }
   ],
   "source": [
    "# Installing Beautifulsoup \n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\nandakishore\\new folder (2)\\lib\\site-packages (4.8.0)\n"
     ]
    }
   ],
   "source": [
    "# Installing lxml file : lxml is the most feature-rich and easy-to-use library for processing XML and HTML in the Python language\n",
    "! pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nandakishore\\new folder (2)\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nandakishore\\new folder (2)\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\nandakishore\\new folder (2)\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nandakishore\\new folder (2)\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\nandakishore\\new folder (2)\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\nandakishore\\new folder (2)\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# Installing nltk Packages\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nltk.download' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Download Packages for nltk\n",
    "!nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs \n",
    "import urllib.request\n",
    "import re\n",
    "scraped_data =  urllib.request.urlopen('https://builtin.com/artificial-intelligence ')\n",
    "article = scraped_data.read()\n",
    "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "para = parsed_article.find_all('p')\n",
    "article_text = \"\"\n",
    "for p in para:\n",
    "  article_text += p.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Text Cleaning\n",
    "processed_article = article_text.lower()\n",
    "processed_article = re.sub('[^a-zA-Z]',' ',processed_article)\n",
    "processed_article = re.sub(r'\\s+',' ',processed_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of Dataset\n",
    "all_sentences = nltk.sent_tokenize(processed_article)\n",
    "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "for i in range (len(all_words)):\n",
    "  all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(all_words, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ai': 0, 'intelligence': 1, 'learning': 2, 'artificial': 3, 'human': 4, 'data': 5, 'machine': 6, 'deep': 7, 'machines': 8, 'use': 9, 'like': 10, 'limited': 11, 'time': 12, 'many': 13, 'self': 14, 'potential': 15, 'making': 16, 'even': 17, 'research': 18, 'well': 19, 'also': 20, 'memory': 21, 'every': 22, 'reactive': 23, 'weak': 24, 'next': 25, 'technology': 26, 'companies': 27, 'based': 28, 'world': 29, 'would': 30, 'strong': 31, 'uses': 32, 'games': 33, 'learn': 34, 'without': 35, 'decisions': 36, 'information': 37, 'type': 38, 'ml': 39, 'cars': 40, 'task': 41, 'development': 42, 'percent': 43, 'mind': 44, 'health': 45, 'may': 46, 'yet': 47, 'industry': 48, 'model': 49, 'billion': 50, 'cases': 51, 'years': 52, 'law': 53, 'designed': 54, 'quest': 55, 'level': 56, 'typically': 57, 'able': 58, 'number': 59, 'neural': 60, 'general': 61, 'input': 62, 'come': 63, 'actually': 64, 'tasks': 65, 'moore': 66, 'decision': 67, 'could': 68, 'advancements': 69, 'found': 70, 'capable': 71, 'smart': 72, 'creating': 73, 'concerned': 74, 'driving': 75, 'user': 76, 'assistants': 77, 'intelligent': 78, 'two': 79, 'patterns': 80, 'computer': 81, 'playing': 82, 'wide': 83, 'massive': 84, 'capabilities': 85, 'improve': 86, 'applied': 87, 'allows': 88, 'bad': 89, 'researchers': 90, 'experts': 91, 'looking': 92, 'difficulty': 93, 'good': 94, 'process': 95, 'believe': 96, 'performing': 97, 'processing': 98, 'speech': 99, 'cognitive': 100, 'generation': 101, 'science': 102, 'siri': 103, 'alexa': 104, 'robots': 105, 'kind': 106, 'humans': 107, 'trained': 108, 'across': 109, 'speaking': 110, 'known': 111, 'distinguish': 112, 'systems': 113, 'perform': 114, 'much': 115, 'several': 116, 'due': 117, 'language': 118, 'blood': 119, 'including': 120, 'patient': 121, 'healthcare': 122, 'identify': 123, 'image': 124, 'assess': 125, 'traffic': 126, 'chatgpt': 127, 'according': 128, 'wearable': 129, 'communicate': 130, 'understand': 131, 'others': 132, 'consciousness': 133, 'future': 134, 'sometime': 135, 'people': 136, 'raised': 137, 'doubling': 138, 'one': 139, 'media': 140, 'major': 141, 'jobs': 142, 'costs': 143, 'idea': 144, 'excited': 145, 'respondents': 146, 'surveyed': 147, 'important': 148, 'entertainment': 149, 'variety': 150, 'made': 151, 'comes': 152, 'field': 153, 'lead': 154, 'banking': 155, 'report': 156, 'business': 157, 'industries': 158, 'make': 159, 'emotions': 160, 'set': 161, 'techniques': 162, 'networks': 163, 'sets': 164, 'expected': 165, 'end': 166, 'output': 167, 'new': 168, 'algorithms': 169, 'better': 170, 'used': 171, 'using': 172, 'impact': 173, 'although': 174, 'examples': 175, 'basic': 176, 'content': 177, 'car': 178, 'specialized': 179, 'equally': 180, 'results': 181, 'react': 182, 'things': 183, 'utilize': 184, 'understanding': 185, 'premise': 186, 'psychological': 187, 'concept': 188, 'reach': 189, 'theory': 190, 'six': 191, 'built': 192, 'created': 193, 'store': 194, 'greater': 195, 'complex': 196, 'essentially': 197, 'way': 198, 'however': 199, 'benefits': 200, 'real': 201, 'past': 202, 'terms': 203}\n"
     ]
    }
   ],
   "source": [
    "vocabulary = word2vec.wv.key_to_index \n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = word2vec.wv['machine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "simillar_words = word2vec.wv.most_similar('learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('could', 0.3873165547847748), ('task', 0.22948245704174042), ('machines', 0.22227203845977783), ('raised', 0.2196233570575714), ('surveyed', 0.21424628794193268), ('years', 0.2139076441526413), ('benefits', 0.2129976600408554), ('basic', 0.20450356602668762), ('limited', 0.19983460009098053), ('real', 0.19963425397872925)]\n"
     ]
    }
   ],
   "source": [
    "print(simillar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
